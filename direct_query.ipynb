{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.00%\n",
      "Precision: 0.67\n",
      "Recall: 0.98\n",
      "F1 Score: 0.80\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "output_dir = 'results/target_docs/old/compare'\n",
    "output_file = f'{output_dir}/direct_query-nfcorpus-llama3-Top15-M250-N15.json'\n",
    "with open(output_file, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "def extract_yes_no(answer):\n",
    "    \"\"\"Extract 'Yes' or 'No' from a response.\"\"\"\n",
    "    match = re.search(r'\\b(Yes|No)\\b', answer, re.IGNORECASE)\n",
    "    return match.group(1).capitalize() if match else \"Invalid\"\n",
    "\n",
    "def load_and_calculate_metrics(data):\n",
    "    expected_answers = []\n",
    "    predicted_answers = []\n",
    "\n",
    "    for doc_id, doc_data in data.items():\n",
    "        expected_answer = \"Yes\" if doc_data[\"mem\"].lower() == \"yes\" else \"No\"\n",
    "        llm_response = extract_yes_no(doc_data[\"llm_responses\"][0])\n",
    "\n",
    "        # Skip invalid responses\n",
    "        if llm_response == \"Invalid\":\n",
    "            continue\n",
    "\n",
    "        expected_answers.append(expected_answer)\n",
    "        predicted_answers.append(llm_response)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(expected_answers, predicted_answers) * 100\n",
    "    precision = precision_score(expected_answers, predicted_answers, pos_label=\"Yes\", zero_division=0)\n",
    "    recall = recall_score(expected_answers, predicted_answers, pos_label=\"Yes\", zero_division=0)\n",
    "    f1 = f1_score(expected_answers, predicted_answers, pos_label=\"Yes\", zero_division=0)\n",
    "\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# Calculate and print metrics\n",
    "accuracy, precision, recall, f1 = load_and_calculate_metrics(data)\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Anderson et al. Metrics:\n",
      "Accuracy: 75.00%\n",
      "Precision: 0.67\n",
      "Recall: 0.98\n",
      "F1 Score: 0.80\n",
      "False Positive Rate (FPR): 0.330\n",
      "True Positive Rate (TPR): 0.983\n",
      "\n",
      "Dataset: NFCORPUS\n",
      "Anderson et al. - FPR: 0.330, TPR: 0.983\n",
      "Ours (Top-15) AUC=0.965 - TPR at Anderson's FPR: 0.995\n",
      "Li et al. AUC=0.749 - TPR at Anderson's FPR: 0.696\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Dataset and results directory\n",
    "output_dir = 'results/target_docs/old/compare'\n",
    "\n",
    "# Check if file exists\n",
    "def file_exists(filepath):\n",
    "    return os.path.isfile(filepath)\n",
    "\n",
    "# Function to extract 'Yes' or 'No' from Anderson et al.'s responses\n",
    "def extract_yes_no(answer):\n",
    "    match = re.search(r'\\b(Yes|No)\\b', answer, re.IGNORECASE)\n",
    "    return match.group(1).capitalize() if match else \"Invalid\"\n",
    "\n",
    "# Function to calculate a single ROC point for Anderson et al.\n",
    "def compute_anderson_point(data):\n",
    "    \"\"\"Compute metrics and return a single ROC point.\"\"\"\n",
    "    expected_answers, predicted_answers = [], []\n",
    "\n",
    "    for doc_id, doc_data in data.items():\n",
    "        expected_answer = 1 if doc_data[\"mem\"].lower() == \"yes\" else 0\n",
    "        llm_response = extract_yes_no(doc_data[\"llm_responses\"][0])\n",
    "        if llm_response == \"Invalid\":\n",
    "            continue\n",
    "\n",
    "        predicted_answer = 1 if llm_response == \"Yes\" else 0\n",
    "        expected_answers.append(expected_answer)\n",
    "        predicted_answers.append(predicted_answer)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(expected_answers, predicted_answers) * 100\n",
    "    precision = precision_score(expected_answers, predicted_answers, zero_division=0)\n",
    "    recall = recall_score(expected_answers, predicted_answers, zero_division=0)\n",
    "    f1 = f1_score(expected_answers, predicted_answers, zero_division=0)\n",
    "\n",
    "    # ROC point\n",
    "    fpr = 1 - precision  # False Positive Rate\n",
    "    tpr = recall  # True Positive Rate\n",
    "\n",
    "    return accuracy, precision, recall, f1, fpr, tpr\n",
    "\n",
    "# Mock function for other attacks' scores\n",
    "def get_mia_scores(data):\n",
    "    all_acc = []\n",
    "    for doc_id, doc_data in data.items():\n",
    "        correct_answers = doc_data['answers']\n",
    "        llm_responses = doc_data['llm_responses']\n",
    "        total_questions = len(correct_answers)\n",
    "        correct_count = 0\n",
    "\n",
    "        # Compare LLM responses with correct answers\n",
    "        for i in range(total_questions):\n",
    "            correct_answer = extract_yes_no(correct_answers[i])\n",
    "            llm_answer = extract_yes_no(llm_responses[i])\n",
    "            if correct_answer == llm_answer:\n",
    "                correct_count += 1\n",
    "\n",
    "        # Calculate accuracy for the document\n",
    "        accuracy = correct_count / total_questions * 100\n",
    "        all_acc.append(accuracy)\n",
    "    return all_acc\n",
    "\n",
    "# Function to plot ROC curves (with or without log scale)\n",
    "def plot_roc_curves(dataset, attacks_data, anderson_point=None, log_scale=False):\n",
    "    \"\"\"Plot TPR vs. FPR with or without log-log axes.\"\"\"\n",
    "    plt.figure(figsize=(4, 3), dpi=200)\n",
    "\n",
    "    for fpr, tpr, label in attacks_data:\n",
    "        plt.plot(fpr, tpr, label=label, alpha=0.7)\n",
    "\n",
    "    # Add Anderson et al.'s point if available\n",
    "    if anderson_point:\n",
    "        acc, prec, rec, f1, fpr, tpr = anderson_point\n",
    "        plt.scatter([fpr], [tpr], color='red', label=f\"Anderson et al.\", zorder=5)\n",
    "\n",
    "    # Set log-log scale if required\n",
    "    if log_scale:\n",
    "        plt.semilogx()\n",
    "        plt.semilogy()\n",
    "        plt.xlim(1e-3, 1)\n",
    "        plt.ylim(1e-3, 1)\n",
    "\n",
    "    # Add axis labels and legend\n",
    "    plt.xlabel(\"False Positive Rate\", fontsize=10)\n",
    "    plt.ylabel(\"True Positive Rate\", fontsize=10)\n",
    "    plt.plot([0, 1], [0, 1], ls='--', color='gray', alpha=0.5)\n",
    "    plt.legend(loc=\"best\", fontsize=8)\n",
    "    plt.title(dataset.upper(), fontsize=12)\n",
    "    plt.savefig(f'{dataset}_{\"log\" if log_scale else \"regular\"}.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Prepare data for plotting\n",
    "datasets = ['nfcorpus', 'trec-covid']\n",
    "roc_data_by_dataset = {dataset: [] for dataset in datasets}\n",
    "anderson_points = {}\n",
    "\n",
    "for dataset in datasets:\n",
    "    # Process Attack 1: Your attack\n",
    "    for top in [15]:\n",
    "        output_file = f'{output_dir}/mia-{dataset}-llama3-Top{top}-M250-N15.json'\n",
    "        if file_exists(output_file):\n",
    "            with open(output_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            all_acc = get_mia_scores(data)\n",
    "            mem_acc = all_acc[:len(all_acc)//2]\n",
    "            nonmem_acc = all_acc[len(all_acc)//2:]\n",
    "\n",
    "            labels = np.concatenate([np.ones(len(mem_acc)), np.zeros(len(nonmem_acc))])\n",
    "            scores = np.concatenate([mem_acc, nonmem_acc])\n",
    "\n",
    "            fpr, tpr, _ = roc_curve(labels, scores)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            roc_data_by_dataset[dataset].append((fpr, tpr, f\"Ours (AUC={roc_auc:.3f})\"))\n",
    "        else:\n",
    "            print(f\"File not found: {output_file}\")\n",
    "\n",
    "    # Process Attack 2: Li et al.\n",
    "    output_file = f'{output_dir}/s2-{dataset}-llama3-Top15-M250-N15.json'\n",
    "    if file_exists(output_file):\n",
    "        with open(output_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        all_acc = [doc_data['bleu_score'] for doc_data in data.values()]\n",
    "        mem_acc = all_acc[:len(all_acc)//2]\n",
    "        nonmem_acc = all_acc[len(all_acc)//2:]\n",
    "\n",
    "        labels = np.concatenate([np.ones(len(mem_acc)), np.zeros(len(nonmem_acc))])\n",
    "        scores = np.concatenate([mem_acc, nonmem_acc])\n",
    "\n",
    "        fpr, tpr, _ = roc_curve(labels, scores)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        roc_data_by_dataset[dataset].append((fpr, tpr, f\"Li et al. (AUC={roc_auc:.3f})\"))\n",
    "    else:\n",
    "        print(f\"File not found: {output_file}\")\n",
    "\n",
    "    # Process Attack 3: Anderson et al.\n",
    "    output_file = f'{output_dir}/direct_query-{dataset}-llama3-Top15-M250-N15.json'\n",
    "    if file_exists(output_file):\n",
    "        with open(output_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        anderson_points[dataset] = compute_anderson_point(data)\n",
    "    else:\n",
    "        print(f\"File not found: {output_file}\")\n",
    "\n",
    "# Plot for each dataset in both regular and log-log format\n",
    "for dataset, roc_data in roc_data_by_dataset.items():\n",
    "    formatted_data = [(fpr, tpr, label) for fpr, tpr, label in roc_data]\n",
    "\n",
    "    # Regular ROC plot\n",
    "    plot_roc_curves(dataset, formatted_data, anderson_points.get(dataset), log_scale=False)\n",
    "\n",
    "    # Log-log ROC plot\n",
    "    plot_roc_curves(dataset, formatted_data, anderson_points.get(dataset), log_scale=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ours (Top-15) - TPR at FPR=0.100%: 0.293\n",
      "Ours (Top-15) - TPR at FPR=1.000%: 0.406\n",
      "Li et al. - TPR at FPR=0.100%: 0.012\n",
      "Li et al. - TPR at FPR=1.000%: 0.072\n",
      "\n",
      "Anderson et al. Metrics:\n",
      "Accuracy: 75.00%\n",
      "Precision: 0.67\n",
      "Recall: 0.98\n",
      "F1 Score: 0.80\n",
      "False Positive Rate (FPR): 0.330\n",
      "True Positive Rate (TPR): 0.983\n",
      "\n",
      "Dataset: NFCORPUS\n",
      "Anderson et al. - FPR: 0.330, TPR: 0.983\n",
      "Ours (Top-15) AUC=0.965 - TPR at Anderson's FPR: 0.995\n",
      "Li et al. AUC=0.749 - TPR at Anderson's FPR: 0.696\n",
      "Ours (Top-15) AUC=0.965 - TPR at FPR=0.100%: 0.293\n",
      "Li et al. AUC=0.749 - TPR at FPR=0.100%: 0.012\n",
      "Ours (Top-15) AUC=0.965 - TPR at FPR=1.000%: 0.406\n",
      "Li et al. AUC=0.749 - TPR at FPR=1.000%: 0.072\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Dataset and results directory\n",
    "output_dir = 'results/target_docs/old/compare'\n",
    "\n",
    "# Check if file exists\n",
    "def file_exists(filepath):\n",
    "    return os.path.isfile(filepath)\n",
    "\n",
    "# Function to extract 'Yes' or 'No' from Anderson et al.'s responses\n",
    "def extract_yes_no(answer):\n",
    "    match = re.search(r'\\b(Yes|No)\\b', answer, re.IGNORECASE)\n",
    "    return match.group(1).capitalize() if match else \"Invalid\"\n",
    "\n",
    "# Function to calculate a single ROC point for Anderson et al.\n",
    "def compute_anderson_point(data):\n",
    "    \"\"\"Compute metrics and return a single ROC point.\"\"\"\n",
    "    expected_answers, predicted_answers = [], []\n",
    "\n",
    "    for doc_id, doc_data in data.items():\n",
    "        expected_answer = 1 if doc_data[\"mem\"].lower() == \"yes\" else 0\n",
    "        llm_response = extract_yes_no(doc_data[\"llm_responses\"][0])\n",
    "        if llm_response == \"Invalid\":\n",
    "            continue\n",
    "\n",
    "        predicted_answer = 1 if llm_response == \"Yes\" else 0\n",
    "        expected_answers.append(expected_answer)\n",
    "        predicted_answers.append(predicted_answer)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(expected_answers, predicted_answers) * 100\n",
    "    precision = precision_score(expected_answers, predicted_answers, zero_division=0)\n",
    "    recall = recall_score(expected_answers, predicted_answers, zero_division=0)\n",
    "    f1 = f1_score(expected_answers, predicted_answers, zero_division=0)\n",
    "\n",
    "    # ROC point\n",
    "    fpr = 1 - precision  # False Positive Rate\n",
    "    tpr = recall  # True Positive Rate\n",
    "\n",
    "    # Print Anderson et al.'s metrics\n",
    "    print(\"\\nAnderson et al. Metrics:\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1 Score: {f1:.2f}\")\n",
    "    print(f\"False Positive Rate (FPR): {fpr:.3f}\")\n",
    "    print(f\"True Positive Rate (TPR): {tpr:.3f}\")\n",
    "\n",
    "    return accuracy, precision, recall, f1, fpr, tpr\n",
    "\n",
    "# Interpolate TPR at specific FPR for other methods\n",
    "def interpolate_tpr(fpr_target, fpr_list, tpr_list):\n",
    "    return np.interp(fpr_target, fpr_list, tpr_list)\n",
    "\n",
    "# Mock function for other attacks' scores\n",
    "def get_mia_scores(data):\n",
    "    all_acc = []\n",
    "    for doc_id, doc_data in data.items():\n",
    "        correct_answers = doc_data['answers']\n",
    "        llm_responses = doc_data['llm_responses']\n",
    "        total_questions = len(correct_answers)\n",
    "        correct_count = 0\n",
    "\n",
    "        # Compare LLM responses with correct answers\n",
    "        for i in range(total_questions):\n",
    "            correct_answer = extract_yes_no(correct_answers[i])\n",
    "            llm_answer = extract_yes_no(llm_responses[i])\n",
    "            if correct_answer == llm_answer:\n",
    "                correct_count += 1\n",
    "\n",
    "        # Calculate accuracy for the document\n",
    "        accuracy = correct_count / total_questions * 100\n",
    "        all_acc.append(accuracy)\n",
    "    return all_acc\n",
    "\n",
    "# Prepare data for plotting and metrics comparison\n",
    "datasets = ['nfcorpus']\n",
    "roc_data_by_dataset = {dataset: [] for dataset in datasets}\n",
    "anderson_points = {}\n",
    "\n",
    "low_fprs = [0.001, 0.01]  # Low FPR thresholds: 0.1% and 1%\n",
    "\n",
    "for dataset in datasets:\n",
    "    # Process Attack 1: Your attack\n",
    "    for top in [15]:\n",
    "        output_file = f'{output_dir}/mia-{dataset}-llama3-Top{top}-M250-N15.json'\n",
    "        if file_exists(output_file):\n",
    "            with open(output_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            all_acc = get_mia_scores(data)\n",
    "            mem_acc = all_acc[:len(all_acc)//2]\n",
    "            nonmem_acc = all_acc[len(all_acc)//2:]\n",
    "\n",
    "            labels = np.concatenate([np.ones(len(mem_acc)), np.zeros(len(nonmem_acc))])\n",
    "            scores = np.concatenate([mem_acc, nonmem_acc])\n",
    "\n",
    "            fpr, tpr, _ = roc_curve(labels, scores)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            roc_data_by_dataset[dataset].append((fpr, tpr, f\"Ours (Top-{top}) AUC={roc_auc:.3f}\"))\n",
    "\n",
    "            # Report TPR at low FPRs\n",
    "            for low_fpr in low_fprs:\n",
    "                tpr_at_low_fpr = interpolate_tpr(low_fpr, fpr, tpr)\n",
    "                print(f\"Ours (Top-{top}) - TPR at FPR={low_fpr:.3%}: {tpr_at_low_fpr:.3f}\")\n",
    "        else:\n",
    "            print(f\"File not found: {output_file}\")\n",
    "\n",
    "    # Process Attack 2: Li et al.\n",
    "    output_file = f'{output_dir}/s2-{dataset}-llama3-Top15-M250-N15.json'\n",
    "    if file_exists(output_file):\n",
    "        with open(output_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        all_acc = [doc_data['bleu_score'] for doc_data in data.values()]\n",
    "        mem_acc = all_acc[:len(all_acc)//2]\n",
    "        nonmem_acc = all_acc[len(all_acc)//2:]\n",
    "\n",
    "        labels = np.concatenate([np.ones(len(mem_acc)), np.zeros(len(nonmem_acc))])\n",
    "        scores = np.concatenate([mem_acc, nonmem_acc])\n",
    "\n",
    "        fpr, tpr, _ = roc_curve(labels, scores)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        roc_data_by_dataset[dataset].append((fpr, tpr, f\"Li et al. AUC={roc_auc:.3f}\"))\n",
    "\n",
    "        # Report TPR at low FPRs\n",
    "        for low_fpr in low_fprs:\n",
    "            tpr_at_low_fpr = interpolate_tpr(low_fpr, fpr, tpr)\n",
    "            print(f\"Li et al. - TPR at FPR={low_fpr:.3%}: {tpr_at_low_fpr:.3f}\")\n",
    "    else:\n",
    "        print(f\"File not found: {output_file}\")\n",
    "\n",
    "    # Process Attack 3: Anderson et al.\n",
    "    output_file = f'{output_dir}/direct_query-{dataset}-llama3-Top15-M250-N15.json'\n",
    "    if file_exists(output_file):\n",
    "        with open(output_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        anderson_points[dataset] = compute_anderson_point(data)\n",
    "    else:\n",
    "        print(f\"File not found: {output_file}\")\n",
    "\n",
    "# Report Anderson et al.'s FPR and TPR, and interpolate TPR for other methods\n",
    "for dataset, roc_data in roc_data_by_dataset.items():\n",
    "    print(f\"\\nDataset: {dataset.upper()}\")\n",
    "\n",
    "    # Report Anderson et al.'s metrics\n",
    "    if dataset in anderson_points:\n",
    "        _, _, _, _, anderson_fpr, anderson_tpr = anderson_points[dataset]\n",
    "        print(f\"Anderson et al. - FPR: {anderson_fpr:.3f}, TPR: {anderson_tpr:.3f}\")\n",
    "\n",
    "        # Interpolate TPR for other methods at Anderson's FPR\n",
    "        for fpr, tpr, label in roc_data:\n",
    "            interpolated_tpr = interpolate_tpr(anderson_fpr, fpr, tpr)\n",
    "            print(f\"{label} - TPR at Anderson's FPR: {interpolated_tpr:.3f}\")\n",
    "\n",
    "        # Report TPR at low FPRs\n",
    "        for low_fpr in low_fprs:\n",
    "            for fpr, tpr, label in roc_data:\n",
    "                tpr_at_low_fpr = interpolate_tpr(low_fpr, fpr, tpr)\n",
    "                print(f\"{label} - TPR at FPR={low_fpr:.3%}: {tpr_at_low_fpr:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
