{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'trec-covid'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ColBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered DataFrame:\n",
      "         id                                              title  \\\n",
      "0  ug7v899j  Clinical features of culture-proven Mycoplasma...   \n",
      "1  02tnwd4m  Nitric oxide: a pro-inflammatory mediator in l...   \n",
      "2  ejv2xln0    Surfactant protein-D and pulmonary host defense   \n",
      "3  9785vg6d  Gene expression in epithelial cells in respons...   \n",
      "4  zjufx4fo  Sequence requirements for RNA strand transfer ...   \n",
      "\n",
      "                                                text  \n",
      "0  OBJECTIVE: This retrospective chart review des...  \n",
      "1  Inflammatory diseases of the respiratory tract...  \n",
      "2  Surfactant protein-D (SP-D) participates in th...  \n",
      "3  Respiratory syncytial virus (RSV) and pneumoni...  \n",
      "4  Nidovirus subgenomic mRNAs contain a leader se...  \n",
      "\n",
      "\n",
      "[Nov 29, 18:03:17] #> Note: Output directory ./datasets/trec-covid/colbert/indexes/trec-covid-index already exists\n",
      "\n",
      "\n",
      "[Nov 29, 18:03:17] #> Will delete 39 files already at ./datasets/trec-covid/colbert/indexes/trec-covid-index in 20 seconds...\n",
      "[Nov 29, 18:03:39] [0] \t\t #> Encoding 58804 passages..\n",
      "[Nov 29, 18:05:58] [0] \t\t avg_doclen_est = 274.2843322753906 \t len(local_sample) = 58,804\n",
      "[Nov 29, 18:06:04] [0] \t\t Creating 65,536 partitions.\n",
      "[Nov 29, 18:06:04] [0] \t\t *Estimated* 30,873,718 embeddings.\n",
      "[Nov 29, 18:06:04] [0] \t\t #> Saving the indexing plan to ./datasets/trec-covid/colbert/indexes/trec-covid-index/plan.json ..\n",
      "Clustering 16079016 points in 128D to 65536 clusters, redo 1 times, 4 iterations\n",
      "  Preprocessing in 1.37 s\n",
      "  Iteration 3 (86.20 s, search 81.01 s): objective=2.96244e+06 imbalance=1.324 nsplit=0       \n",
      "[Nov 29, 18:07:45] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Nov 29, 18:07:45] Loading packbits_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[0.029, 0.03, 0.031, 0.028, 0.029, 0.031, 0.032, 0.028, 0.028, 0.029, 0.03, 0.028, 0.03, 0.031, 0.028, 0.031, 0.027, 0.029, 0.028, 0.028, 0.031, 0.03, 0.029, 0.031, 0.029, 0.029, 0.031, 0.03, 0.029, 0.032, 0.029, 0.033, 0.031, 0.028, 0.029, 0.027, 0.031, 0.03, 0.029, 0.036, 0.031, 0.029, 0.029, 0.03, 0.028, 0.03, 0.029, 0.033, 0.031, 0.028, 0.028, 0.03, 0.031, 0.029, 0.029, 0.029, 0.035, 0.03, 0.034, 0.03, 0.028, 0.032, 0.03, 0.032, 0.03, 0.03, 0.031, 0.032, 0.029, 0.031, 0.03, 0.027, 0.03, 0.032, 0.028, 0.031, 0.032, 0.03, 0.032, 0.032, 0.032, 0.029, 0.03, 0.032, 0.028, 0.03, 0.029, 0.031, 0.027, 0.032, 0.029, 0.032, 0.029, 0.03, 0.03, 0.031, 0.034, 0.029, 0.029, 0.029, 0.031, 0.033, 0.032, 0.029, 0.032, 0.03, 0.031, 0.028, 0.031, 0.029, 0.03, 0.03, 0.031, 0.027, 0.031, 0.029, 0.03, 0.03, 0.029, 0.03, 0.029, 0.03, 0.032, 0.031, 0.028, 0.032, 0.03, 0.029]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Nov 29, 18:07:46] [0] \t\t #> Encoding 25000 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [01:01, 61.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Nov 29, 18:08:47] [0] \t\t #> Encoding 25000 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [02:03, 61.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Nov 29, 18:09:50] [0] \t\t #> Encoding 25000 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [03:04, 61.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Nov 29, 18:10:51] [0] \t\t #> Encoding 25000 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [04:05, 61.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Nov 29, 18:11:52] [0] \t\t #> Encoding 12561 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [04:36, 55.36s/it]\n",
      "100%|██████████| 5/5 [00:00<00:00, 17.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Nov 29, 18:12:27] #> Optimizing IVF to store map from centroids to list of pids..\n",
      "[Nov 29, 18:12:27] #> Building the emb2pid mapping..\n",
      "[Nov 29, 18:12:27] len(emb2pid) = 30859788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65536/65536 [00:03<00:00, 20010.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Nov 29, 18:12:31] #> Saved optimized IVF to ./datasets/trec-covid/colbert/indexes/trec-covid-index/ivf.pid.pt\n",
      "Done indexing!\n",
      "Indexing completed. Index saved at: datasets/trec-covid/colbert/indexes/trec-covid-index\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from ragatouille import RAGPretrainedModel\n",
    "\n",
    "# Dataset paths\n",
    "corpus_file = f'./datasets/{dataset}/corpus.json'\n",
    "selected_indices_file = f'./datasets/{dataset}/selected_indices.json'\n",
    "output_dir = f'./datasets/{dataset}'\n",
    "\n",
    "# Step 1: Load the corpus\n",
    "with open(corpus_file, 'r') as file:\n",
    "    corpus = json.load(file)\n",
    "\n",
    "# Step 2: Load selected indices (non-members to exclude)\n",
    "with open(selected_indices_file, \"r\") as f:\n",
    "    selected_indices = json.load(f)  # Assuming it contains \"mem_indices\" and \"non_mem_indices\"\n",
    "\n",
    "# Step 3: Filter the corpus to exclude non-members\n",
    "non_mem_indices = set(selected_indices[\"non_mem_indices\"])\n",
    "filtered_corpus = {doc_id: content for doc_id, content in corpus.items() if doc_id not in non_mem_indices}\n",
    "\n",
    "# Step 4: Transform the filtered corpus into a pandas DataFrame\n",
    "records = []\n",
    "for doc_id, content in filtered_corpus.items():\n",
    "    records.append({\n",
    "        'id': doc_id,\n",
    "        'title': content.get('title', ''),\n",
    "        'text': content.get('text', '')\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# Ensure the DataFrame contains the required fields\n",
    "print(\"Filtered DataFrame:\")\n",
    "print(df.head())\n",
    "\n",
    "# Step 5: Prepare the data for indexing\n",
    "document_ids = df['id'].tolist()\n",
    "my_documents = df['text'].tolist()\n",
    "document_metadatas = [{\"title\": title} for title in df['title']]\n",
    "\n",
    "# Step 6: Load the pretrained RAG model\n",
    "model_name = \"colbert-ir/colbertv2.0\"  # Replace with your actual model\n",
    "RAG = RAGPretrainedModel.from_pretrained(model_name, index_root=output_dir)\n",
    "\n",
    "# Step 7: Index the documents\n",
    "index_name = f\"{dataset}-index\"\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(os.path.join(os.path.dirname(output_dir), 'colbert', 'indexes', index_name), exist_ok=True)\n",
    "\n",
    "index_path = RAG.index(\n",
    "    index_name=index_name,\n",
    "    collection=my_documents,\n",
    "    document_ids=document_ids,\n",
    "    document_metadatas=document_metadatas,\n",
    "    max_document_length = 512\n",
    ")\n",
    "print(f\"Indexing completed. Index saved at: {index_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading searcher for index trec-covid-index for the first time... This may take a few seconds\n",
      "[Nov 29, 18:13:56] #> Loading codec...\n",
      "[Nov 29, 18:13:56] #> Loading IVF...\n",
      "[Nov 29, 18:13:56] #> Loading doclens...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 831.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Nov 29, 18:13:56] #> Loading codes and residuals...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searcher loaded!\n",
      "\n",
      "#> QueryTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n",
      "#> Input: . Can donor and acceptor template homology facilitate recombinogenic transfers?, \t\t True, \t\t None\n",
      "#> Output IDs: torch.Size([32]), tensor([  101,     1,  2064, 15009,  1998,  5138,  2953, 23561, 24004,  6483,\n",
      "        10956, 28667,  5358, 21891, 16505, 15210,  1029,   102,   103,   103,\n",
      "          103,   103,   103,   103,   103,   103,   103,   103,   103,   103,\n",
      "          103,   103], device='cuda:0')\n",
      "#> Output Mask: torch.Size([32]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
      "\n",
      "{'content': 'Abstract Reverse transcription requires two replicative template switches, called minus and plus strand strong stop transfer, and can include additional, recombinogenic switches. Donor and acceptor template homology facilitates both replicative and recombinogenic transfers, but homology-independent determinants may also contribute. Here, improved murine leukemia virus-based assays were established and the effects of varying extents of mismatches and complementarity between primer and acceptor template regions were assessed. Template switch accuracy was addressed by examining provirus structures, and efficiency was measured using a competitive titer assay. The results demonstrated that limited mismatch extension occurred readily during both minus and plus strand transfer. A strong bias for correct targeting to the U3/R junction and against use of alternate regions of homology was observed during minus strand transfer. Transfer to the U3/R junction was as accurate with five bases of complementarity as it was with an intact R, and as few as 3nt targeted transfer to a limited extent. In contrast, 12 base recombinogenic acceptors were utilized poorly and no accurate switch was observed when recombination acceptors retained only five bases of complementarity. These findings confirm that murine leukemia virus replicative and recombinogenic template switches differ in homology requirements, and support the notion that factors other than primer–template complementarity may contribute to strong stop acceptor template recognition.', 'score': 24.65625, 'rank': 1, 'document_id': '47okncdw', 'passage_id': 82463, 'document_metadata': {'title': 'Mismatch Extension During Strong Stop Strand Transfer and Minimal Homology Requirements for Replicative Template Switching During Moloney Murine Leukemia Virus Replication'}}\n",
      "{'content': 'Genetic recombination is an important mechanism of retrovirus variation and diversity. Size variation in the surface (SU) glycoprotein, characterized by duplication and insertion, has been observed during in vivo infection with several lentiviruses, including bovine immunodeficiency virus (BIV), equine infectious anaemia virus (EIAV) and human immunodeficiency virus type 1. These duplication/insertion events are thought to occur through a mechanism of template switching/strand transfer during reverse transcription. Studies of RNA recombination in a number of virus systems indicate that cis-acting sequences can modulate the frequency of template switching/strand transfer. The size variable region of EIAV and BIV SU glycoproteins was examined and an AU-rich region and regions of nucleotide sequence identity that may facilitate template switching/strand transfer were identified. An in vitro strand transfer assay using donor and acceptor templates derived from the size variable region in BIV env detected both precise and imprecise strand transfer products, in addition to full-length products. Sequence analysis of clones obtained from imprecise strand transfer products showed that 87.5% had crossover sites within 10 nt of the crossover site observed in vivo. Mutations in the donor template which altered either the AU-rich region or nucleotide sequence identity dramatically decreased the frequency of imprecise strand transfer. Together, these results suggest that cis-acting elements can modulate non-homologous recombination events during reverse transcription and may contribute to the genetic and biological diversity of lentiviruses in vivo.', 'score': 20.09375, 'rank': 2, 'document_id': '4syiccmk', 'passage_id': 57496, 'document_metadata': {'title': 'Cis-acting sequences may contribute to size variation in the surface glycoprotein of bovine immunodeficiency virus.'}}\n",
      "{'content': \"Molecular mechanisms of RNA recombination were studied in turnip crinkle carmovirus (TCV), which has a uniquely high recombination frequency and non-random crossover site distribution among the recombining TCV-associated satellite RNAs. To test the previously proposed replicase-driven template-switching mechanism for recombination, a partially purified TCV replicase preparation (RdRp) was programed with RNAs resembling the putative in vivo recombination intermediates. Analysis of the in vitro RdRp products revealed efficient generation of 3'-terminal extension products. Initiation of 3'-terminal extension occurred at or close to the base of a hairpin that was a recombination hotspot in vivo. Efficient generation of the 3'-terminal extension products depended on two factors: (i) a hairpin structure in the acceptor RNA region and (ii) a short base-paired region formed between the acceptor RNA and the nascent RNA synthesized from the donor RNA template. The hairpin structure bound to the RdRp, and thus is probably involved in its recruitment. The probable role of the base-paired region is to hold the 3' terminus near the RdRp bound to the hairpin structure to facilitate 3'-terminal extension. These regions were also required for in vivo RNA recombination between TCV-associated sat-RNA C and sat-RNA D, giving crucial and direct support for a replicase-driven template-switching mechanism of RNA recombination.\", 'score': 17.53125, 'rank': 3, 'document_id': 'sfue9fdb', 'passage_id': 81562, 'document_metadata': {'title': 'Dissecting RNA recombination in vitro: role of RNA sequences and the viral replicase.'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from ragatouille import RAGPretrainedModel\n",
    "\n",
    "# Paths to ColBERT index and configuration\n",
    "index_path = os.path.join(\"./datasets\", dataset, 'colbert', 'indexes', f\"{dataset}-index\")\n",
    "model_name = \"colbert-ir/colbertv2.0\"\n",
    "\n",
    "# Load RAG model\n",
    "colbert = RAGPretrainedModel.from_index(index_path)\n",
    "\n",
    "# Example query\n",
    "query = \"Can donor and acceptor template homology facilitate recombinogenic transfers?\"\n",
    "# Perform the search\n",
    "k = 3  # Number of results to retrieve\n",
    "results = colbert.search(query, k=k)\n",
    "\n",
    "# Display the search results\n",
    "for result in results:\n",
    "    print(result)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Not used"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contriever"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/pi_ahoumansadr_umass_edu/yuefeng/conda/envs/rag_mia/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading corpus...\n",
      "Loading selected indices...\n",
      "Filtered corpus size: 105520\n",
      "Encoding documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding batches: 100%|██████████| 413/413 [12:19<00:00,  1.79s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing completed. Index saved to: ./datasets/trec-covid/contriever/indexes/trec-covid-index\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import torch\n",
    "import faiss\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Paths\n",
    "dataset_folder = f\"./datasets/{dataset}\"\n",
    "corpus_file = os.path.join(dataset_folder, \"corpus.json\")\n",
    "selected_indices_file = os.path.join(dataset_folder, \"selected_indices.json\")  # Path to non-member indices\n",
    "index_folder = os.path.join(dataset_folder, \"contriever\", \"indexes\", f\"{dataset}-index\")\n",
    "os.makedirs(index_folder, exist_ok=True)\n",
    "\n",
    "# Model setup\n",
    "model_name = \"facebook/contriever-msmarco\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Move to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Mean pooling function\n",
    "def mean_pooling(token_embeddings, mask):\n",
    "    \"\"\"\n",
    "    Perform mean pooling for sentence embeddings.\n",
    "    \"\"\"\n",
    "    token_embeddings = token_embeddings.masked_fill(~mask[..., None].bool(), 0.)\n",
    "    sentence_embeddings = token_embeddings.sum(dim=1) / mask.sum(dim=1)[..., None]\n",
    "    return sentence_embeddings\n",
    "\n",
    "# Document encoding function\n",
    "def encode_documents(corpus, tokenizer, model, device, batch_size=32):\n",
    "    \"\"\"\n",
    "    Encode documents using Contriever model.\n",
    "    \"\"\"\n",
    "    doc_ids = list(corpus.keys())\n",
    "    texts = [f\"{corpus[doc_id]['title']} {corpus[doc_id]['text']}\" for doc_id in doc_ids]\n",
    "    embeddings = []\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Encoding batches\", unit=\"batch\"):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            pooled_embeddings = mean_pooling(outputs.last_hidden_state, inputs['attention_mask'])\n",
    "            pooled_embeddings = torch.nn.functional.normalize(pooled_embeddings, p=2, dim=1)  # Normalize embeddings\n",
    "\n",
    "        embeddings.append(pooled_embeddings.cpu())\n",
    "\n",
    "    embeddings = torch.cat(embeddings, dim=0).numpy()\n",
    "    return doc_ids, embeddings\n",
    "\n",
    "# Query encoding function\n",
    "def encode_query(query, tokenizer, model, device):\n",
    "    \"\"\"\n",
    "    Encode a single query using Contriever.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(query, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        query_embedding = mean_pooling(outputs.last_hidden_state, inputs['attention_mask'])\n",
    "        query_embedding = torch.nn.functional.normalize(query_embedding, p=2, dim=1)\n",
    "    return query_embedding.cpu().numpy()\n",
    "\n",
    "# Load corpus\n",
    "print(\"Loading corpus...\")\n",
    "with open(corpus_file, \"r\") as f:\n",
    "    corpus = json.load(f)\n",
    "\n",
    "# Load selected indices (exclude non-members)\n",
    "print(\"Loading selected indices...\")\n",
    "with open(selected_indices_file, \"r\") as f:\n",
    "    selected_indices = json.load(f)\n",
    "non_mem_indices = set(selected_indices[\"non_mem_indices\"])\n",
    "\n",
    "# Filter the corpus to exclude non-members\n",
    "filtered_corpus = {doc_id: content for doc_id, content in corpus.items() if doc_id not in non_mem_indices}\n",
    "print(f\"Filtered corpus size: {len(filtered_corpus)}\")\n",
    "\n",
    "# Encode documents\n",
    "print(\"Encoding documents...\")\n",
    "doc_ids, doc_embeddings = encode_documents(filtered_corpus, tokenizer, model, device, batch_size=256)\n",
    "\n",
    "# Save document embeddings using FAISS\n",
    "dimension = doc_embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)  # Inner product index\n",
    "index.add(doc_embeddings)\n",
    "\n",
    "# Save FAISS index and document IDs\n",
    "faiss_index_path = os.path.join(index_folder, \"corpus_index.faiss\")\n",
    "doc_ids_path = os.path.join(index_folder, \"doc_ids.pkl\")\n",
    "\n",
    "faiss.write_index(index, faiss_index_path)\n",
    "with open(doc_ids_path, \"wb\") as f:\n",
    "    pickle.dump(doc_ids, f)\n",
    "\n",
    "print(f\"Indexing completed. Index saved to: {index_folder}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding query: PURPOSE OF REVIEW: Effective acute pain management has evolved considerably in recent years and is a primary area of focus in attempts to defend against the opioid epidemic. Persistent postsurgical pain (PPP) has an incidence of up to 30–50% and has negative outcome of quality of life and negative burden on individuals, family, and society. The 2016 American Society of Anesthesiologists (ASA) guidelines states that enhanced recovery after surgery (ERAS) forms an integral part of Perioperative Surgical Home (PSH) and is now recommended to use a multimodal opioid-sparing approach for management of postoperative pain. As such, dexmedetomidine is now being used as part of ERAS protocols along with regional nerve blocks and other medications, to create a satisfactory postoperative outcome with reduced opioid consumption in the Post anesthesia care unit (PACU). RECENT FINDINGS: Dexmedetomidine, a selective alpha(2) agonist, possesses analgesic effects and has a different mechanism of action when compared with opioids. When dexmedetomidine is initiated at the end of a procedure, it has a better hemodynamic stability and pain response than ropivacaine. Dexmedetomidine can be used as an adjuvant in epidurals with local anesthetic sparing effects. Its use during nerve blocks results in reduced postoperative pain. Also, local infiltration of IV dexmedetomidine is associated with earlier discharge from PACU. SUMMARY: Perioperative use of dexmedetomidine has significantly improved postoperative outcomes when used as part of ERAS protocols. An in-depth review of the use of dexmedetomidine in ERAS protocols is presented for clinical anesthesiologist\n",
      "Performing search for top-3 results...\n",
      "\n",
      "Search Results:\n",
      "Rank 1:\n",
      "  Doc ID: yzv43e8l\n",
      "  Title: Dexmedetomidine in Enhanced Recovery After Surgery (ERAS) Protocols for Postoperative Pain\n",
      "  Text Snippet: PURPOSE OF REVIEW: Effective acute pain management has evolved considerably in recent years and is a primary area of focus in attempts to defend against the opioid epidemic. Persistent postsurgical pa...\n",
      "Rank 2:\n",
      "  Doc ID: 96w3ygrv\n",
      "  Title: Role of dexmedetomidine infusion after coronary artery bypass grafting\n",
      "  Text Snippet: BACKGROUND: Postoperative pain has negative consequences on patients’ outcomes after cardiac surgery. Routine management with opioid and or non-steroidal anti-inflammatory medications has several disa...\n",
      "Rank 3:\n",
      "  Doc ID: lliix0hj\n",
      "  Title: Comparison of dexmedetomidine and dexamethasone for prevention of postoperative nausea and vomiting after laparoscopic cholecystectomy.\n",
      "  Text Snippet: BACKGROUND Postoperative nausea and vomiting (PONV) are common following laparoscopic cholecystectomy (LC). Dexamethasone has been reported to reduce PONV. However, there is insufficient evidence rega...\n"
     ]
    }
   ],
   "source": [
    "# Query retrieval test\n",
    "query = \"PURPOSE OF REVIEW: Effective acute pain management has evolved considerably in recent years and is a primary area of focus in attempts to defend against the opioid epidemic. Persistent postsurgical pain (PPP) has an incidence of up to 30\\u201350% and has negative outcome of quality of life and negative burden on individuals, family, and society. The 2016 American Society of Anesthesiologists (ASA) guidelines states that enhanced recovery after surgery (ERAS) forms an integral part of Perioperative Surgical Home (PSH) and is now recommended to use a multimodal opioid-sparing approach for management of postoperative pain. As such, dexmedetomidine is now being used as part of ERAS protocols along with regional nerve blocks and other medications, to create a satisfactory postoperative outcome with reduced opioid consumption in the Post anesthesia care unit (PACU). RECENT FINDINGS: Dexmedetomidine, a selective alpha(2) agonist, possesses analgesic effects and has a different mechanism of action when compared with opioids. When dexmedetomidine is initiated at the end of a procedure, it has a better hemodynamic stability and pain response than ropivacaine. Dexmedetomidine can be used as an adjuvant in epidurals with local anesthetic sparing effects. Its use during nerve blocks results in reduced postoperative pain. Also, local infiltration of IV dexmedetomidine is associated with earlier discharge from PACU. SUMMARY: Perioperative use of dexmedetomidine has significantly improved postoperative outcomes when used as part of ERAS protocols. An in-depth review of the use of dexmedetomidine in ERAS protocols is presented for clinical anesthesiologist\"\n",
    "print(f\"Encoding query: {query}\")\n",
    "query_embedding = encode_query(query, tokenizer, model, device)\n",
    "\n",
    "# Perform retrieval\n",
    "k = 3  # Number of top results\n",
    "print(f\"Performing search for top-{k} results...\")\n",
    "distances, indices = index.search(query_embedding, k)\n",
    "\n",
    "# Map results back to corpus\n",
    "print(\"\\nSearch Results:\")\n",
    "for rank, idx in enumerate(indices[0], start=1):\n",
    "    doc_id = doc_ids[idx]\n",
    "    doc_content = corpus[doc_id]\n",
    "    print(f\"Rank {rank}:\")\n",
    "    print(f\"  Doc ID: {doc_id}\")\n",
    "    print(f\"  Title: {doc_content.get('title', '')}\")\n",
    "    print(f\"  Text Snippet: {doc_content.get('text', '')[:200]}...\")  # Display snippet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qwen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/pi_ahoumansadr_umass_edu/yuefeng/conda/envs/rag_mia/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "A new version of the following files was downloaded from https://huggingface.co/Alibaba-NLP/gte-Qwen2-1.5B-instruct:\n",
      "- modeling_qwen.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "Downloading shards: 100%|██████████| 2/2 [02:51<00:00, 85.53s/it] \n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:20<00:00, 10.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading corpus...\n",
      "Encoding documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding batches: 100%|██████████| 414/414 [2:20:04<00:00, 20.30s/batch]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing completed. Index saved to: ./datasets/trec-covid/qwen/indexes/trec-covid-index\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import torch\n",
    "import faiss\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Paths\n",
    "dataset_folder = f\"./datasets/{dataset}\"\n",
    "corpus_file = os.path.join(dataset_folder, \"corpus.json\")\n",
    "index_folder = os.path.join(dataset_folder, \"qwen\", \"indexes\", f\"{dataset}-index\")\n",
    "os.makedirs(index_folder, exist_ok=True)\n",
    "\n",
    "# Model setup\n",
    "model_name = \"Alibaba-NLP/gte-Qwen2-1.5B-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_name, trust_remote_code=True).to(\"cuda\")\n",
    "model.eval()\n",
    "\n",
    "# Pooling function\n",
    "def last_token_pool(last_hidden_states: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Pool the last token embeddings based on the attention mask.\n",
    "    \"\"\"\n",
    "    sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "    batch_size = last_hidden_states.shape[0]\n",
    "    return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\n",
    "\n",
    "# Normalization function\n",
    "def normalize_embeddings(embeddings):\n",
    "    return F.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "# Document encoding function\n",
    "def encode_documents(corpus, tokenizer, model, batch_size=256):\n",
    "    \"\"\"\n",
    "    Encode documents using Qwen.\n",
    "    \"\"\"\n",
    "    doc_ids = list(corpus.keys())\n",
    "    texts = [f\"{corpus[doc_id]['title']} {corpus[doc_id]['text']}\" for doc_id in doc_ids]\n",
    "    embeddings = []\n",
    "\n",
    "    print(\"Encoding documents...\")\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Encoding batches\", unit=\"batch\"):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(\"cuda\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            pooled_embeddings = last_token_pool(outputs.last_hidden_state, inputs['attention_mask'])\n",
    "            normalized_embeddings = normalize_embeddings(pooled_embeddings)\n",
    "\n",
    "        embeddings.append(normalized_embeddings.cpu())\n",
    "\n",
    "    embeddings = torch.cat(embeddings, dim=0).numpy()  # Combine and convert to numpy\n",
    "    return doc_ids, embeddings\n",
    "\n",
    "# Load corpus\n",
    "print(\"Loading corpus...\")\n",
    "with open(corpus_file, \"r\") as f:\n",
    "    corpus = json.load(f)\n",
    "\n",
    "# Encode documents\n",
    "doc_ids, doc_embeddings = encode_documents(corpus, tokenizer, model)\n",
    "\n",
    "# Save document embeddings using FAISS\n",
    "dimension = doc_embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)  # Inner product index\n",
    "index.add(doc_embeddings)\n",
    "\n",
    "# Save FAISS index and document IDs\n",
    "faiss_index_path = os.path.join(index_folder, \"corpus_index.faiss\")\n",
    "doc_ids_path = os.path.join(index_folder, \"doc_ids.pkl\")\n",
    "\n",
    "faiss.write_index(index, faiss_index_path)\n",
    "with open(doc_ids_path, \"wb\") as f:\n",
    "    pickle.dump(doc_ids, f)\n",
    "\n",
    "print(f\"Indexing completed. Index saved to: {index_folder}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FAISS index...\n",
      "Loading document IDs...\n",
      "Loading corpus...\n",
      "Encoding queries...\n",
      "Performing search for top-3 results...\n",
      "\n",
      "Search Results:\n",
      "\n",
      "Query: A total of 50 poultry farms of commercial broilers (N = 39) and commercial layers (N = 11) suffered from respiratory problems and mortality during the period\n",
      "  Rank 1:\n",
      "    Doc ID: dfx9z2g7\n",
      "    Title: Seroprevalence of major avian respiratory diseases in broiler and sonali chicken in selected areas of Bangladesh\n",
      "    Text Snippet: OBJECTIVE: This study was conducted to investigate different respiratory diseases in broiler and sonali birds in some selected districts of Bangladesh. MATERIALS AND METHODS: We were collected a total...\n",
      "  Rank 2:\n",
      "    Doc ID: dnswqp4j\n",
      "    Title: Molecular survey and interaction of common respiratory pathogens in chicken flocks (field perspective)\n",
      "    Text Snippet: AIM: The present study was designed for the detection of the most prevalent respiratory infections in chicken flocks and clarifying their interaction and impact on flock health. MATERIALS AND METHODS:...\n",
      "  Rank 3:\n",
      "    Doc ID: or65v4bw\n",
      "    Title: Co-infections, genetic, and antigenic relatedness of avian influenza H5N8 and H5N1 viruses in domestic and wild birds in Egypt\n",
      "    Text Snippet: A total of 50 poultry farms of commercial broilers (N = 39) and commercial layers (N = 11) suffered from respiratory problems and mortality during the period from January 2016 to December 2017 were in...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import torch\n",
    "import faiss\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Paths\n",
    "dataset_folder = f\"./datasets/{dataset}\"\n",
    "index_folder = os.path.join(dataset_folder, \"qwen\", \"indexes\", f\"{dataset}-index\")\n",
    "corpus_file = os.path.join(dataset_folder, \"corpus.json\")\n",
    "\n",
    "# Paths to the saved FAISS index and document IDs\n",
    "faiss_index_path = os.path.join(index_folder, \"corpus_index.faiss\")\n",
    "doc_ids_path = os.path.join(index_folder, \"doc_ids.pkl\")\n",
    "\n",
    "# Model setup\n",
    "model_name = \"Alibaba-NLP/gte-Qwen2-1.5B-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_name, trust_remote_code=True).to(\"cuda\")\n",
    "model.eval()\n",
    "\n",
    "# Pooling function\n",
    "def last_token_pool(last_hidden_states: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "    sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "    batch_size = last_hidden_states.shape[0]\n",
    "    return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\n",
    "\n",
    "# Normalization function\n",
    "def normalize_embeddings(embeddings):\n",
    "    return F.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "# Query encoding function\n",
    "def encode_query(queries, tokenizer, model):\n",
    "    \"\"\"\n",
    "    Encode a list of queries using Qwen.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(queries, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        query_embeddings = last_token_pool(outputs.last_hidden_state, inputs['attention_mask'])\n",
    "        query_embeddings = normalize_embeddings(query_embeddings)\n",
    "    return query_embeddings.cpu().numpy()\n",
    "\n",
    "# Load FAISS index and document IDs\n",
    "print(\"Loading FAISS index...\")\n",
    "faiss_index = faiss.read_index(faiss_index_path)\n",
    "\n",
    "print(\"Loading document IDs...\")\n",
    "with open(doc_ids_path, \"rb\") as f:\n",
    "    doc_ids = pickle.load(f)\n",
    "\n",
    "# Load corpus for mapping doc IDs to text\n",
    "print(\"Loading corpus...\")\n",
    "with open(corpus_file, \"r\") as f:\n",
    "    corpus = json.load(f)\n",
    "\n",
    "# Query retrieval test\n",
    "queries = queries = [\n",
    "            \"A total of 50 poultry farms of commercial broilers (N = 39) and commercial layers (N = 11) suffered from respiratory problems and mortality during the period\"\n",
    "        ]\n",
    "print(\"Encoding queries...\")\n",
    "query_embeddings = encode_query(queries, tokenizer, model)\n",
    "\n",
    "# Perform retrieval\n",
    "k = 3  # Number of top results\n",
    "print(f\"Performing search for top-{k} results...\")\n",
    "distances, indices = faiss_index.search(query_embeddings, k)\n",
    "\n",
    "# Map results back to corpus\n",
    "print(\"\\nSearch Results:\")\n",
    "for query_idx, query in enumerate(queries):\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    for rank, idx in enumerate(indices[query_idx], start=1):\n",
    "        doc_id = doc_ids[idx]\n",
    "        doc_content = corpus.get(doc_id, {})\n",
    "        title = doc_content.get(\"title\", \"No Title\")\n",
    "        text_snippet = doc_content.get(\"text\", \"No Text\")[:200]  # Display snippet\n",
    "        print(f\"  Rank {rank}:\")\n",
    "        print(f\"    Doc ID: {doc_id}\")\n",
    "        print(f\"    Title: {title}\")\n",
    "        print(f\"    Text Snippet: {text_snippet}...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
